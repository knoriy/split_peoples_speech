{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import textgrids\n",
    "import tqdm\n",
    "from utils import flac_to_wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare MFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean old wavs ONLY FOR TESTING\n",
    "import glob\n",
    "\n",
    "old_wav_path = glob.glob(f'/home/knoriy/Documents/laion/split_peoples_speech/mini_subset/**/*.wav', recursive=True)\n",
    "for wav_path in old_wav_path:\n",
    "    os.remove(wav_path)\n",
    "\n",
    "# old_txt_path = glob.glob(f'/home/knoriy/Documents/laion/split_peoples_speech/subset/**/*.txt', recursive=True)\n",
    "# for txt_path in old_txt_path:\n",
    "#     os.remove(txt_path)\n",
    "\n",
    "# old_txt_path = glob.glob(f'/home/knoriy/Documents/laion/split_peoples_speech/subset_split/**/*.txt', recursive=True)\n",
    "# for txt_path in old_txt_path:\n",
    "#     os.remove(txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] load tar\n",
    "- [ ] Split tar into smaller chunks\n",
    "- [ ] save chunk to unique file\n",
    "- [ ] upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files: 100%|██████████| 10/10 [00:00<00:00, 2137.99it/s]\n",
      "Extracting Files: 100%|██████████| 10/10 [00:00<00:00, 1819.50it/s]\n",
      "Extracting Files: 100%|██████████| 10/10 [00:00<00:00, 1759.13it/s]\n",
      "Extracting Files: 100%|██████████| 10/10 [00:00<00:00, 1728.90it/s]\n",
      "Extracting Files: 100%|██████████| 10/10 [00:00<00:00, 1811.25it/s]\n",
      "Extracting Files: 100%|██████████| 10/10 [00:00<00:00, 1869.29it/s]\n",
      "Extracting Files: 100%|██████████| 10/10 [00:00<00:00, 1910.24it/s]\n",
      "Extracting Files: 100%|██████████| 10/10 [00:00<00:00, 1846.25it/s]\n",
      "Extracting Files: 100%|██████████| 10/10 [00:00<00:00, 1946.76it/s]\n",
      "Extracting Files: 100%|██████████| 10/10 [00:00<00:00, 1916.43it/s]\n",
      "Extracting Files: 100%|██████████| 2/2 [00:00<00:00, 1557.19it/s]\n",
      "Chunks remaining: 100%|██████████| 11/11 [00:00<00:00, 63.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import glob\n",
    "import os\n",
    "import tqdm\n",
    "import shutil \n",
    "\n",
    "import os\n",
    "\n",
    "def make_tarfile(source_dir, output_filename):\n",
    "    if not os.path.exists(os.path.dirname(output_filename)):\n",
    "        os.makedirs(os.path.dirname(output_filename))\n",
    "    with tarfile.open(output_filename, \"w\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "def split_large_tar(src_tar:str, dest_path:str, chunk_size:int):\n",
    "    with tarfile.open(src_tar, mode='r') as src_file_obj:\n",
    "        file_names_full_list = src_file_obj.getnames()\n",
    "\n",
    "        for i in tqdm.tqdm(range(0, len(file_names_full_list), chunk_size), desc='Chunks remaining: '):\n",
    "            for file_name in tqdm.tqdm(file_names_full_list[i:i + chunk_size], desc=\"Extracting Files: \"):\n",
    "                    src_file_obj.extract(file_name, f'./tmp/')\n",
    "            \n",
    "            make_tarfile( './tmp/', f'{dest_path}/{i}.tar')\n",
    "            shutil.rmtree('./tmp/')\n",
    "\n",
    "split_large_tar('/home/knoriy/split_peoples_speech/test.tar', \n",
    "'/home/knoriy/split_peoples_speech/test_split/',\n",
    "10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "with tarfile.open('/home/knoriy/split_peoples_speech/test_split/40.tar', mode='r') as src_file_obj:\n",
    "    file_names_full_list = src_file_obj.getnames()\n",
    "    print(len(file_names_full_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tar loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "- [ ] Download and extract single or batch of files from aws\n",
    "- [ ] Process for MFA\n",
    "- [ ] Generate textgrid\n",
    "- [ ] Split audio and upload to aws s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "nohup sh -c \"wget https://the-peoples-speech-west-europe.bj.bcebos.com/part-00000-07a8f0d3-6d27-4299-887a-dc12a6d72f8d-c000.tar?authorization=bce-auth-v1/0ef6765c1e494918bc0d4c3ca3e5c6d1/2021-12-03T06%3A30%3A22Z/-1/host/444b9c082ceffd10f38bb965679ed9ec12202836831e111dd193fde281062d26 -O - | aws s3 cp - s3://s-laion/peoples_speech/train_clean_pps.tar\" &;\n",
    "nohup sh -c \"wget https://the-peoples-speech-west-europe.bj.bcebos.com/part-00000-4e132642-c01c-4db6-9db0-a1e19193f6f8-c000.json?authorization=bce-auth-v1/0ef6765c1e494918bc0d4c3ca3e5c6d1/2021-12-03T06%3A31%3A22Z/-1/host/d7dacf3c31d2e3670d82727636ce234be27a9128df7a80883b84b4a3d8c7f6c0 -O - | aws s3 cp - s3://s-laion/peoples_speech/Manifest.json\" &;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "nohup sh -c \"wget https://storage.googleapis.com/public-datasets-mswc/mswc.tar.gz -O - | aws s3 cp - s3://s-laion/multilingual_spoken_words/full_mswc.tar\" > full_mswc.out &\n",
    "nohup sh -c \"wget https://storage.googleapis.com/public-datasets-mswc/metadata.json.gz -O - | aws s3 cp - s3://s-laion/multilingual_spoken_words/metadata.tar\" > mswc_meta.out &\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# nohup sh -c \"wget https://storage.googleapis.com/public-datasets-mswc/mswc.tar.gz -O - | aws s3 cp - s3://s-laion/multilingual_spoken_words/full_mswc.tar\" > full_mswc.out &\n",
    "# nohup sh -c \"wget https://storage.googleapis.com/public-datasets-mswc/metadata.json.gz -O - | aws s3 cp - s3://s-laion/multilingual_spoken_words/metadata.tar\" > mswc_meta.out &\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22598391661e11fb86220d8e605c241deaf9a8263a5dee63ccb3c05da51e658c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('aligner')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e21918bae0143b5a633fbf1b4c6b5406aaf2f0d47fb4c4c8eb32f4c836e66610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
