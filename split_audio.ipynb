{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import textgrids\n",
    "import tqdm\n",
    "from utils import flac_to_wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Subset json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep files in subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "def get_subset_df(json_dir=None):\n",
    "    if not json_dir:\n",
    "        json_dir = '/home/knoriy/Documents/laion/split_peoples_speech/flac_train_manifest.jsonl'\n",
    "\n",
    "    with open(json_dir, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(json_list)\n",
    "    df[0] = df[0].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    for keys in df[0][0].keys():\n",
    "        df[keys] = [path[0][keys] for path in df.iloc()]\n",
    "\n",
    "    subset = glob.glob('/home/knoriy/Documents/laion/split_peoples_speech/subset/**/*.flac', recursive=True)\n",
    "    subset = [os.path.join(*(dir.split(os.path.sep)[7:])) for dir in subset]\n",
    "\n",
    "    subset_df = df[df['audio_filepath'].isin(subset)].reset_index(drop=True)\n",
    "    subset_df = subset_df.drop(0, axis=1)\n",
    "\n",
    "    return subset_df\n",
    "\n",
    "get_subset_df().to_csv('/home/knoriy/Documents/laion/split_peoples_speech/subset.tsv', sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare MFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean old wavs ONLY FOR TESTING\n",
    "import glob\n",
    "\n",
    "old_wav_path = glob.glob(f'/home/knoriy/Documents/laion/split_peoples_speech/subset/**/*.wav', recursive=True)\n",
    "for wav_path in old_wav_path:\n",
    "    os.remove(wav_path)\n",
    "\n",
    "# old_txt_path = glob.glob(f'/home/knoriy/Documents/laion/split_peoples_speech/subset/**/*.txt', recursive=True)\n",
    "# for txt_path in old_txt_path:\n",
    "#     os.remove(txt_path)\n",
    "\n",
    "# old_txt_path = glob.glob(f'/home/knoriy/Documents/laion/split_peoples_speech/subset_split/**/*.txt', recursive=True)\n",
    "# for txt_path in old_txt_path:\n",
    "#     os.remove(txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tar loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "- [ ] Download and extract single or batch of files from aws\n",
    "- [ ] Process for MFA\n",
    "- [ ] Generate textgrid\n",
    "- [ ] Split audio and upload to aws s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/home/knoriy/Documents/laion/split_peoples_speech/subset_flac.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset\n",
      "subset/gov_DOT_uscourts_DOT_ca5_DOT_16-41264\n",
      "subset/gov_DOT_uscourts_DOT_ca5_DOT_16-41264/gov_DOT_uscourts_DOT_ca5_DOT_16-41264_DOT_2017-10-04_DOT_mp3_00051.flac\n",
      "subset/gov_DOT_uscourts_DOT_ca9_DOT_13-73491\n",
      "subset/gov_DOT_uscourts_DOT_ca9_DOT_13-73491/gov_DOT_uscourts_DOT_ca9_DOT_13-73491_DOT_2017-11-14_DOT_mp3_00008.flac\n",
      "subset/snafuinfinityJournal011819-CL19\n",
      "subset/snafuinfinityJournal011819-CL19/Journal011819-CL19_DOT_mp3_00068.flac\n",
      "subset/snafuinfinityJournal011819-CL19/Journal011819-CL19_DOT_mp3_00046.flac\n",
      "subset/Talking_Politics_in_Western_PA_-_Lisa_Anderson_Peters_Township_Board_of_School_Directors\n",
      "subset/Talking_Politics_in_Western_PA_-_Lisa_Anderson_Peters_Township_Board_of_School_Directors/Talking_Politics_in_Western_PA_-_Lisa_Anderson_Peters_Township_Board_of_School_Directors_DOT_HD_DOT_mp3_00026.flac\n"
     ]
    }
   ],
   "source": [
    "with tarfile.open(filename,'r') as file_obj:\n",
    "    file_names = file_obj.getnames()[:10]\n",
    "\n",
    "    for file_name in file_names:\n",
    "        os.makedirs(os.path.join('Test', os.path.split(file_name)[0]), exist_ok=True)\n",
    "        file_obj.extract(file_name, './')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import tarfile\n",
    "\n",
    "class S3File(io.BytesIO):\n",
    "    def __init__(self, bucket_name, key_name, s3client):\n",
    "        super().__init__()\n",
    "        self.bucket_name = bucket_name\n",
    "        self.key_name = key_name\n",
    "        self.s3client = s3client\n",
    "        self.offset = 0\n",
    "\n",
    "    def close(self):\n",
    "        return\n",
    "\n",
    "    def read(self, size):\n",
    "        print('read: offset = {}, size = {}'.format(self.offset, size))\n",
    "        start = self.offset\n",
    "        end = self.offset + size - 1\n",
    "        try:\n",
    "            s3_object = self.s3client.get_object(Bucket=self.bucket_name, Key=self.key_name, Range=\"bytes=%d-%d\" % (start, end))\n",
    "        except:\n",
    "            return bytearray()\n",
    "        self.offset = self.offset + size\n",
    "        result = s3_object['Body'].read()\n",
    "        return result\n",
    "\n",
    "    def seek(self, offset, whence=0):\n",
    "        if whence == 0:\n",
    "            print('seek: offset {} -> {}'.format(self.offset, offset))\n",
    "            self.offset = offset\n",
    "\n",
    "    def tell(self):\n",
    "        return self.offset\n",
    "\n",
    "s3file = S3File(bucket_name, file_name, s3client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "nohup sh -c \"wget https://the-peoples-speech-west-europe.bj.bcebos.com/part-00000-07a8f0d3-6d27-4299-887a-dc12a6d72f8d-c000.tar?authorization=bce-auth-v1/0ef6765c1e494918bc0d4c3ca3e5c6d1/2021-12-03T06%3A30%3A22Z/-1/host/444b9c082ceffd10f38bb965679ed9ec12202836831e111dd193fde281062d26 -O - | aws s3 cp - s3://s-laion/peoples_speech/train_clean_pps.tar\" &;\n",
    "nohup sh -c \"wget https://the-peoples-speech-west-europe.bj.bcebos.com/part-00000-4e132642-c01c-4db6-9db0-a1e19193f6f8-c000.json?authorization=bce-auth-v1/0ef6765c1e494918bc0d4c3ca3e5c6d1/2021-12-03T06%3A31%3A22Z/-1/host/d7dacf3c31d2e3670d82727636ce234be27a9128df7a80883b84b4a3d8c7f6c0 -O - | aws s3 cp - s3://s-laion/peoples_speech/Manifest.json\" &;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "nohup sh -c \"wget https://storage.googleapis.com/public-datasets-mswc/mswc.tar.gz -O - | aws s3 cp - s3://s-laion/multilingual_spoken_words/full_mswc.tar\" > full_mswc.out &\n",
    "nohup sh -c \"wget https://storage.googleapis.com/public-datasets-mswc/metadata.json.gz -O - | aws s3 cp - s3://s-laion/multilingual_spoken_words/metadata.tar\" > mswc_meta.out &\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# nohup sh -c \"wget https://storage.googleapis.com/public-datasets-mswc/mswc.tar.gz -O - | aws s3 cp - s3://s-laion/multilingual_spoken_words/full_mswc.tar\" > full_mswc.out &\n",
    "# nohup sh -c \"wget https://storage.googleapis.com/public-datasets-mswc/metadata.json.gz -O - | aws s3 cp - s3://s-laion/multilingual_spoken_words/metadata.tar\" > mswc_meta.out &\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('aligner')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e21918bae0143b5a633fbf1b4c6b5406aaf2f0d47fb4c4c8eb32f4c836e66610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
