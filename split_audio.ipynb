{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import textgrids\n",
    "import pandas as pd\n",
    "\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import shutil\n",
    "import fsspec\n",
    "from utils import generate_txt, get_subset_df, genorate_pps_df, make_tarfile, create_json_list\n",
    "\n",
    "chunk = 2\n",
    "generate_subset_tsv = True\n",
    "pps_df_dir = '/home/knoriy/split_peoples_speech/pps_train.tsv'\n",
    "\n",
    "root_path = '/home/knoriy/split_peoples_speech/'\n",
    "dataset_name = 'test'\n",
    "\n",
    "metadata_dir = \"/mnt/knoriy/metadata.json\"\n",
    "tar_dir = \"/mnt/knoriy/pps_train.tar\"\n",
    "\n",
    "# init Dirs\n",
    "dataset_root_path = os.path.join(root_path, f'{dataset_name}')\n",
    "dataset_textgrid_path = os.path.join(root_path, f'{dataset_name}_textgrids')\n",
    "dataset_split_path = os.path.join(root_path, f'{dataset_name}_split')\n",
    "s3 = fsspec.filesystem('s3')\n",
    "s3_dest = f's-laion/peoples_speech/{dataset_name}_tars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(pps_df_dir):\n",
    "    pps_df = pd.read_csv(pps_df_dir, sep='\\t', header=None, names=['audio_filepath', 'text'])\n",
    "else:\n",
    "    pps_df = genorate_pps_df(metadata_dir)\n",
    "    pps_df.to_csv(pps_df_dir, sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_filepath</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gov_DOT_uscourts_DOT_ca9_DOT_04-56618/gov_DOT_...</td>\n",
       "      <td>is there anything to wear it on this one or ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gov_DOT_uscourts_DOT_ca9_DOT_04-56618/gov_DOT_...</td>\n",
       "      <td>i thought the law was that we don't have juris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gov_DOT_uscourts_DOT_ca9_DOT_04-56618/gov_DOT_...</td>\n",
       "      <td>review it yes you do yes you do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gov_DOT_uscourts_DOT_ca9_DOT_04-56618/gov_DOT_...</td>\n",
       "      <td>this you know what happened there was a single...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gov_DOT_uscourts_DOT_ca9_DOT_04-56618/gov_DOT_...</td>\n",
       "      <td>motion to abandon the quantity because nothing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4320997</th>\n",
       "      <td>The_Capture_movie/Capture_DOT_mp3_00011.flac</td>\n",
       "      <td>reward of all our marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4320998</th>\n",
       "      <td>The_Capture_movie/Capture_DOT_mp3_00012.flac</td>\n",
       "      <td>so i traveled back the way i'd come on it this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4320999</th>\n",
       "      <td>The_Capture_movie/Capture_DOT_mp3_00013.flac</td>\n",
       "      <td>you state your business mister</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4321000</th>\n",
       "      <td>The_Capture_movie/Capture_DOT_mp3_00014.flac</td>\n",
       "      <td>come out yet i don't want to be just that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4321001</th>\n",
       "      <td>The_Capture_movie/Capture_DOT_mp3_00015.flac</td>\n",
       "      <td>we're going to hire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4321002 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            audio_filepath  \\\n",
       "0        gov_DOT_uscourts_DOT_ca9_DOT_04-56618/gov_DOT_...   \n",
       "1        gov_DOT_uscourts_DOT_ca9_DOT_04-56618/gov_DOT_...   \n",
       "2        gov_DOT_uscourts_DOT_ca9_DOT_04-56618/gov_DOT_...   \n",
       "3        gov_DOT_uscourts_DOT_ca9_DOT_04-56618/gov_DOT_...   \n",
       "4        gov_DOT_uscourts_DOT_ca9_DOT_04-56618/gov_DOT_...   \n",
       "...                                                    ...   \n",
       "4320997       The_Capture_movie/Capture_DOT_mp3_00011.flac   \n",
       "4320998       The_Capture_movie/Capture_DOT_mp3_00012.flac   \n",
       "4320999       The_Capture_movie/Capture_DOT_mp3_00013.flac   \n",
       "4321000       The_Capture_movie/Capture_DOT_mp3_00014.flac   \n",
       "4321001       The_Capture_movie/Capture_DOT_mp3_00015.flac   \n",
       "\n",
       "                                                      text  \n",
       "0        is there anything to wear it on this one or ha...  \n",
       "1        i thought the law was that we don't have juris...  \n",
       "2                          review it yes you do yes you do  \n",
       "3        this you know what happened there was a single...  \n",
       "4        motion to abandon the quantity because nothing...  \n",
       "...                                                    ...  \n",
       "4320997                         reward of all our marriage  \n",
       "4320998  so i traveled back the way i'd come on it this...  \n",
       "4320999                     you state your business mister  \n",
       "4321000          come out yet i don't want to be just that  \n",
       "4321001                                we're going to hire  \n",
       "\n",
       "[4321002 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening file: This may take some time\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_file_obj = tarfile.open(tar_dir, mode='r')\n",
    "print('opening file: This may take some time\\n')\n",
    "\n",
    "file_names_full_list = src_file_obj.getnames()\n",
    "file_names_full_list = [i for i in file_names_full_list if '.flac' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Files: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s]\n",
      "Generating .txt files for MFA: 100%|██████████| 2/2 [00:00<00:00, 1231.26it/s]\n",
      "Converting .flac files to .wav: 100%|██████████| 2/2 [00:00<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Setting up corpus information...\n",
      "INFO - Loading corpus from source files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.96it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Found 1 speaker across 2 files, average number of utterances per speaker: 2.0\n",
      "INFO - Initializing multiprocessing jobs...\n",
      "WARNING - Number of jobs was specified as 3, but due to only having 1 speakers, MFA will only use 1 jobs.\n",
      "INFO - Creating corpus split for feature generation...\n",
      "INFO - Generating base features (mfcc)...\n",
      "INFO - Generating MFCCs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:01<?, ?it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Calculating CMVN...\n",
      "INFO - Creating corpus split with features...\n",
      "INFO - Compiling training graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.21it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Performing first-pass alignment...\n",
      "INFO - Generating alignments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Calculating fMLLR for speaker adaptation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Performing second-pass alignment...\n",
      "INFO - Generating alignments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Exporting TextGrids to /home/knoriy/split_peoples_speech/test_textgrids...\n",
      "INFO - Collecting phone and word alignments from alignment lattices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:10<00:00,  5.28s/it]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Finished exporting TextGrids to /home/knoriy/split_peoples_speech/test_textgrids!\n",
      "INFO - Done! Everything took 32.29901075363159 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spliting flac files into 5-10 seconds: 100%|██████████| 2/2 [00:00<00:00,  3.49it/s]\n",
      "Chunks remaining:   0%|          | 0/2160501 [00:37<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(0, len(file_names_full_list), chunk), desc='Chunks remaining: '):\n",
    "    for file_name in tqdm.tqdm(file_names_full_list[i:i + chunk], desc=\"Extracting Files: \"):\n",
    "        src_file_obj.extract(file_name, f'./{dataset_name}/')\n",
    "\n",
    "    if generate_subset_tsv == True:\n",
    "        df = get_subset_df(f'{dataset_root_path}/**/*.flac', pps_df)\n",
    "\n",
    "    save_all_text_to_file(df, dataset_name)\n",
    "\n",
    "    # Convert Flac to wav\n",
    "    convert_all_to_wav(df, os.path.join(root_path, dataset_name))\n",
    "\n",
    "    # Get audio text alignments and split audio\n",
    "    generate_textgrids(os.path.join(root_path, dataset_name))\n",
    "    split_all_audio_files(dataset_textgrid_path, dataset_root_path)\n",
    "\n",
    "    # # Upload Split files to s3\n",
    "    tar_file_path = make_tarfile(f'{dataset_split_path}', f'{dataset_root_path}/{i}.tar')\n",
    "    # s3.put(tar_file_path, os.path.join(s3_dest, os.path.basename(tar_file_path)))\n",
    "    # print('File Uploaded to: ', os.path.join(s3_dest, os.path.basename(tar_file_path)))\n",
    "\n",
    "    # # Upload sizes.jsonl to s3\n",
    "    # create_json_list(json_sizes_path, {'filename':tar_file_path, 'num_samples':len(df)})\n",
    "    # s3.put(json_sizes_path, 's-laion/peoples_speech/sizes.jsonl')\n",
    "\n",
    "    # shutil.rmtree(dataset_root_path)\n",
    "    # shutil.rmtree(dataset_textgrid_path)\n",
    "    # shutil.rmtree(dataset_split_path)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "nohup sh -c \"wget https://the-peoples-speech-west-europe.bj.bcebos.com/part-00000-07a8f0d3-6d27-4299-887a-dc12a6d72f8d-c000.tar?authorization=bce-auth-v1/0ef6765c1e494918bc0d4c3ca3e5c6d1/2021-12-03T06%3A30%3A22Z/-1/host/444b9c082ceffd10f38bb965679ed9ec12202836831e111dd193fde281062d26 -O - | aws s3 cp - s3://s-laion/peoples_speech/train_clean_pps.tar\" &;\n",
    "nohup sh -c \"wget https://the-peoples-speech-west-europe.bj.bcebos.com/part-00000-4e132642-c01c-4db6-9db0-a1e19193f6f8-c000.json?authorization=bce-auth-v1/0ef6765c1e494918bc0d4c3ca3e5c6d1/2021-12-03T06%3A31%3A22Z/-1/host/d7dacf3c31d2e3670d82727636ce234be27a9128df7a80883b84b4a3d8c7f6c0 -O - | aws s3 cp - s3://s-laion/peoples_speech/Manifest.json\" &;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import json\n",
    "\n",
    "def tardir(file_path, tar_name, n_entry_each, audio_ext=\".flac\", text_ext=\".json\", shuffle=True, start_idx=0, delete_file=False):\n",
    "    \"\"\"\n",
    "    This function create the tars that includes the audio and text files in the same folder\n",
    "    @param file_path      | string  | the path where audio and text files located\n",
    "    @param tar_name       | string  | the tar name\n",
    "    @param n_entry_each   | int     | how many pairs of (audio, text) will be in a tar\n",
    "    @param audio_ext      | string  | the extension of the audio\n",
    "    @param text_ext       | string  | the extension of the text\n",
    "    @param shuffle        | boolean | True to shuffle the file sequence before packing up\n",
    "    @param start_idx      | int     | the start index of the tar\n",
    "    @param delete_file    | boolean | True to delete the audio and text files after packing up\n",
    "    \"\"\"\n",
    "    filelist = glob(file_path+'/*'+audio_ext)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(filelist)\n",
    "    count = 0\n",
    "    n_split = len(filelist) // n_entry_each\n",
    "    if n_split * n_entry_each != len(filelist):\n",
    "        n_split += 1\n",
    "    size_dict = {\n",
    "        os.path.basename(tar_name) + str(i) + \".tar\": n_entry_each\n",
    "        for i in range(n_split)\n",
    "    }\n",
    "    if n_split * n_entry_each != len(filelist):\n",
    "        size_dict[os.path.basename(tar_name) + str(n_split - 1) + \".tar\"] = (\n",
    "            len(filelist) - (n_split - 1) * n_entry_each\n",
    "        )\n",
    "    for i in tqdm(range(start_idx, n_split + start_idx)):\n",
    "        with tarfile.open(tar_name + str(i) + \".tar\", \"w\") as tar_handle:\n",
    "            for j in range(count, len(filelist)):\n",
    "                audio = filelist[j]\n",
    "                basename = \".\".join(audio.split(\".\")[:-1])\n",
    "                text_file_path = os.path.join(file_path, basename + text_ext)\n",
    "                audio_file_path = os.path.join(file_path, audio)\n",
    "                tar_handle.add(audio_file_path)\n",
    "                tar_handle.add(text_file_path)\n",
    "                if delete_file:\n",
    "                    os.remove(audio_file_path)\n",
    "                    os.remove(text_file_path)\n",
    "                if (j + 1) % n_entry_each == 0:\n",
    "                    count = j + 1\n",
    "                    break\n",
    "        tar_handle.close()\n",
    "    # Serializing json\n",
    "    json_object = json.dumps(size_dict, indent=4)\n",
    "    # Writing to sample.json\n",
    "    with open(os.path.join(os.path.dirname(tar_name), \"sizes.json\"), \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    return size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22598391661e11fb86220d8e605c241deaf9a8263a5dee63ccb3c05da51e658c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('aligner')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e21918bae0143b5a633fbf1b4c6b5406aaf2f0d47fb4c4c8eb32f4c836e66610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
